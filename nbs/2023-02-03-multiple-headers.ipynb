{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1: for and concat\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "B, T, C = 2, 4, 8 # batch, time, channel\n",
    "q = torch.randn((B, T, C)) # B, T, C\n",
    "\n",
    "k = torch.randn((B, T, C))\n",
    "\n",
    "v = torch.randn((B, T, C))\n",
    "\n",
    "def head(q, k, v):\n",
    "    att = (q @ k.transpose(-2, -1)) / C**0.5 # B, T, T\n",
    "    tril = torch.tril(torch.ones(T, T)) # T, T\n",
    "    att = att.masked_fill(tril == 0, float('-inf')) # B, T, T\n",
    "    att = torch.softmax(att, dim=-1) # B, T, T\n",
    "\n",
    "    out = att @ v # B, T, C\n",
    "    return out\n",
    "\n",
    "out = head(q, k, v) # B, T, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2582, -2.0407, -0.8016, -0.8183, -1.1820, -0.2877, -0.6043,  0.6002],\n",
       "        [-1.4053, -0.5922, -0.2548,  1.1517, -0.0179,  0.4264, -0.7657, -0.0545],\n",
       "        [-1.2743,  0.4513, -0.2280,  0.9224,  0.2056, -0.4970,  0.5821,  0.2053],\n",
       "        [-0.3018, -0.6703, -0.6171, -0.8334,  0.4839, -0.1349,  0.2119, -0.8714]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2582, -2.0407, -0.8016, -0.8183, -1.1820, -0.2877, -0.6043,  0.6002],\n",
       "        [-0.5085, -1.7247, -0.6823, -0.3885, -0.9280, -0.1319, -0.6395,  0.4574],\n",
       "        [-1.2056, -0.2033, -0.3026,  0.8066, -0.0315, -0.1442, -0.0328,  0.1576],\n",
       "        [-0.8482, -0.1931, -0.4107,  0.1548,  0.2657, -0.2460,  0.2601, -0.2675]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(v[0][0], out[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(0.7818 * v[0][0] + 0.2182 * v[0][1], out[0][1], atol=1e-02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1, q2 = torch.randn((B, T, C)), torch.randn((B, T, C)) # B, T, C\n",
    "\n",
    "k1, k2 = torch.randn((B, T, C)), torch.randn((B, T, C))\n",
    "\n",
    "v1, v2 = torch.randn((B, T, C)), torch.randn((B, T, C))\n",
    "\n",
    "head1 = head(q1, k1, v1) # B, T, C\n",
    "head2 = head(q2, k2, v2) # B, T, C\n",
    "\n",
    "heads = torch.cat([head1, head2], -1) # B, T, 2*C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(head1, heads[:, :, :C]), torch.allclose(head2, heads[:, :, C:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2 multiple\n",
    "a = torch.arange(B*T*C*2).view(2, B, T, C).float()\n",
    "b = torch.ones(2, B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a @ b.transpose(-1, -2) # 2, B, T, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 28.,  28.,  28.,  28.],\n",
       "          [ 92.,  92.,  92.,  92.],\n",
       "          [156., 156., 156., 156.],\n",
       "          [220., 220., 220., 220.]],\n",
       "\n",
       "         [[284., 284., 284., 284.],\n",
       "          [348., 348., 348., 348.],\n",
       "          [412., 412., 412., 412.],\n",
       "          [476., 476., 476., 476.]]],\n",
       "\n",
       "\n",
       "        [[[540., 540., 540., 540.],\n",
       "          [604., 604., 604., 604.],\n",
       "          [668., 668., 668., 668.],\n",
       "          [732., 732., 732., 732.]],\n",
       "\n",
       "         [[796., 796., 796., 796.],\n",
       "          [860., 860., 860., 860.],\n",
       "          [924., 924., 924., 924.],\n",
       "          [988., 988., 988., 988.]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c0 = a[0] @ b[0].transpose(-1, -2) # B, T, T\n",
    "c1 = a[1] @ b[1].transpose(-1, -2) # B, T, T\n",
    "torch.stack([c0, c1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(c, torch.stack([c0, c1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.stack([q1, q2], 1) # B, 2, T, C\n",
    "k = torch.stack([k1, k2], 1) # B, 2, T, C\n",
    "v = torch.stack([v1, v2], 1) # B, 2, T, C\n",
    "\n",
    "heads2 = head(q, k, v)\n",
    "\n",
    "torch.allclose(heads[:,:,0:C], heads2[0])\n",
    "torch.allclose(heads[:,:,C:], heads2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(heads ,heads2.transpose(1,2).reshape(B, T, C*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multiple_head(q, k, v):\n",
    "    # shape B, n_head, T, C\n",
    "    att = (q @ k.transpose(-2, -1)) / C**0.5 # B, n_head, T, T\n",
    "    tril = torch.tril(torch.ones(T, T)) # T, T\n",
    "    att = att.masked_fill(tril == 0, float('-inf')) # B, n_head, T, T\n",
    "    att = torch.softmax(att, dim=-1) # B, n_head, T, T\n",
    "\n",
    "    out = att @ v # B, n_head, T, C\n",
    "    out = out.transpose(1,2).reshape(B, T, -1) # B, T, C * n_head\n",
    "    return out\n",
    "\n",
    "heads3 = multiple_head(q, k, v)\n",
    "torch.allclose(heads ,heads3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.stack([torch.ones(B*T*C).view(B, T, C), torch.ones(B*T*C).view(B, T, C) * 2.0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.transpose(1,2).reshape(B, T, C*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1., 1., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2.]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(C, head_size)\n",
    "        self.key = nn.Linear(C, head_size)\n",
    "        self.value = nn.Linear(C, head_size)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(T, T)))\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q = self.query(x) # B, T, head_size\n",
    "        k = self.key(x) # B, T, head_size\n",
    "        v = self.value(x) # B, T, head_size\n",
    "        # computer attention score\n",
    "        wei = q @ v.transpose(-2, -1) * head_size ** -0.5 # (B, T, head_size) x (B, head_size, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = torch.softmax(wei,dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation\n",
    "        out = wei@v # (B, T, T) x (B, T, head_size) -> (B, T, head_size)\n",
    "        return out\n",
    "\n",
    "class MultipleHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        n_embd = head_size * num_heads\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], -1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleHeadAttention_v2(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(C, head_size * num_heads)\n",
    "        self.key = nn.Linear(C, head_size * num_heads)\n",
    "        self.value = nn.Linear(C, head_size * num_heads)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(T, T)))\n",
    "        self.proj = nn.Linear(num_heads*head_size, num_heads*head_size)\n",
    "        self.dropout1 = nn.Dropout(drop_out)\n",
    "        self.dropout2 = nn.Dropout(drop_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x = x.view(B, T, C)\n",
    "        q = self.query(x).view(B, T, num_heads, head_size).transpose(1,2) # B, num_heads, T, head_size\n",
    "        k = self.key(x).view(B, T, num_heads, head_size).transpose(1,2) # B, num_heads, T, head_size\n",
    "        v = self.value(x).view(B, T, num_heads, head_size).transpose(1,2) # B, num_heads, T, head_size\n",
    "        # computer attention score\n",
    "        wei = q @ v.transpose(-2, -1) * head_size ** -0.5 # (B, num_heads, T, head_size) x (B, num_heads, head_size, T) -> (B, num_heads, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = torch.softmax(wei,dim=-1) # (B, num_heads, T, T)\n",
    "        wei = self.dropout1(wei)\n",
    "        # perform the weighted aggregation\n",
    "        out = wei@v # (B, num_heads, T, T) x (B, num_heads, T, head_size) -> (B, num_heads, T, head_size)\n",
    "        out = out.transpose(1,2).reshape(B, T, -1) # B, T, head_size * n_head\n",
    "        out = self.dropout2(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, C = 4, 8, 32\n",
    "head_size = 16\n",
    "num_heads = int(32 / head_size)\n",
    "x = torch.randn(B, T, C)\n",
    "Q1 = torch.randn(C, head_size)\n",
    "Q2 = torch.randn(C, head_size)\n",
    "\n",
    "q1 = x @ Q1 # (B, T, C) @ (C, head_size) -> (B, T, head_size)\n",
    "q2 = x @ Q2 # (B, T, C) @ (C, head_size) -> (B, T, head_size)\n",
    "\n",
    "q = torch.stack([q1, q2], 1) # B, 2, T, head_size\n",
    "assert(torch.allclose(q1, q[:,0,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.cat([Q1, Q2], 1) # C, head_size * num_heads\n",
    "q = x @ Q # B, T, head_size * num_heads\n",
    "q = q.view(B, T, num_heads, head_size).transpose(1,2) # B, num_heads, T, head_size\n",
    "assert torch.allclose(q1, q[:, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.stack([Q1, Q2]) # num_heads, C, head_size\n",
    "q_v2 = x.view(B, 1, T, C) @ Q # B, num_heads, T, head_size\n",
    "\n",
    "assert(torch.allclose(q, q_v2, atol=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_out = 0\n",
    "num_heads = 2\n",
    "att = MultipleHeadAttention_v2(head_size)\n",
    "\n",
    "out = att(x)\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert this file to md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import Javascript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('this_notebook = \"' + IPython.notebook.notebook_name + '\"')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "IPython.notebook.kernel.execute('this_notebook = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-02-03-multiple-headers.ipynb'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook 2023-02-03-multiple-headers.ipynb to markdown\n",
      "[NbConvertApp] Writing 9248 bytes to ../_posts/2023-02-03-multiple-headers.md\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to markdown {this_notebook} --output-dir=../_posts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
