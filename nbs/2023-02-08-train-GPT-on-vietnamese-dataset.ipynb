{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74caba21",
   "metadata": {},
   "source": [
    "# 2023-01-30 train GPT on vietnamese dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "816f7896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c46933ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83ee6ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read on review data\n",
    "with open('../data/truyen_kieu.csv', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    text = text.replace(',', ' , ')\n",
    "    text = [o for sent in text.split(\"\\n\") for o in sent.split(\".\") if not o.isnumeric()]\n",
    "    text = ' \\n '.join(text)\n",
    "    text = text.lower()\n",
    "    truyen_kieu = text.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91188271",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = truyen_kieu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8251df1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trăm', 'năm', 'trong', 'cõi', 'người', 'ta', ',', '', '\\n', 'chữ', 'tài', 'chữ', 'mệnh', 'khéo', 'là', 'ghét', 'nhau', '\\n', '', '\\n', 'trải', 'qua', 'một', 'cuộc', 'bể', 'dâu', '\\n', 'những', 'điều', 'trông', 'thấy', 'mà', 'đau', 'đớn', 'lòng', '\\n', '', '\\n', 'lạ', 'gì', 'bỉ', 'sắc', 'tư', 'phong', ',', '', '\\n', 'trời', 'xanh', 'quen', 'thói', 'má', 'hồng', 'đánh', 'ghen', '\\n', '', '\\n', 'cảo', 'thơm', 'lần', 'giở', 'trước', 'đèn', ',', '', '\\n', 'phong', 'tình', 'cổ', 'lục', 'còn', 'truyền', 'sử', 'xanh', '\\n', '', '\\n', 'rằng', 'năm', 'gia', 'tĩnh', 'triều', 'minh', ',', '', '\\n', 'bốn', 'phương', 'phẳng', 'lặng', ',', '', 'hai', 'kinh', 'vững', 'vàng', '\\n', '', '\\n', 'có', 'nhà', 'viên', 'ngoại', 'họ', 'vương', ',', '', '\\n', 'gia', 'tư', 'nghĩ', 'cũng', 'thường', 'thường', 'bực', 'trung', '\\n', '', '\\n', 'một', 'trai', 'con', 'thứ', 'rốt', 'lòng', ',', '', '\\n', 'vương', 'quan', 'là', 'chữ', ',', '', 'nối', 'dòng', 'nho', 'gia', '\\n', '', '\\n', 'đầu', 'lòng', 'hai', 'ả', 'tố', 'nga', ',', '', '\\n', 'thúy', 'kiều', 'là', 'chị', ',', '', 'em', 'là', 'thúy', 'vân', '\\n', '', '\\n', 'mai', 'cốt', 'cách', ',', '', 'tuyết', 'tinh', 'thần', ',', '', '\\n', 'mỗi', 'người', 'một', 'vẻ', ',', '', 'mười', 'phân', 'vẹn', 'mười', '\\n', '', '\\n', 'vân', 'xem', 'trang', 'trọng', 'khác', 'vời', ',', '', '\\n', 'khuôn', 'trăng', 'đầy', 'đặn', ',', '', 'nét', 'ngài', 'nở', 'nang', '\\n', '', '\\n', 'hoa', 'cười', 'ngọc', 'thốt', 'đoan', 'trang', ',', '', '\\n', 'mây', 'thua', 'nước', 'tóc', ',', '', 'tuyết', 'nhường', 'màu', 'da', '\\n', '', '\\n', 'kiều', 'càng', 'sắc', 'sảo', ',', '', 'mặn', 'mà', ',', '', '\\n', 'so', 'bề', 'tài', ',', '', 'sắc', ',', '', 'lại', 'là', 'phần', 'hơn', '\\n', '', '\\n', 'làn', 'thu', 'thủy', ',', '', 'nét', 'xuân', 'sơn', ',', '', '\\n', 'hoa', 'ghen', 'thua', 'thắm', ',', '', 'liễu', 'hờn', 'kém', 'xanh', '\\n', '', '\\n', 'một', ',', '', 'hai', 'nghiêng', 'nước', 'nghiêng', 'thành', ',', '', '\\n', 'sắc', 'đành', 'đòi', 'một', ',', '', 'tài', 'đành', 'họa', 'hai', '\\n', '', '\\n', 'thông', 'minh', 'vốn', 'sẵn', 'tư', 'trời', ',', '', '\\n', 'pha', 'nghề', 'thi', 'họa', ',', '', 'đủ', 'mùi', 'ca', 'ngâm', '\\n', '', '\\n', 'cung', 'thương']\n"
     ]
    }
   ],
   "source": [
    "print(text[0:330])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88afc58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2175, 936]\n",
      "tôi là\n"
     ]
    }
   ],
   "source": [
    "# Here is all unique character that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "ctoi = {c: i for i, c in enumerate(chars)}\n",
    "itoc = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [ctoi[c] for c in s if c in ctoi]\n",
    "decode = lambda l: ' '.join([itoc[i] for i in l])\n",
    "\n",
    "print(encode(\"tôi là\".split(\" \")))\n",
    "\n",
    "print(decode(encode(\"tôi là\".split(\" \"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1db819cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32186]) torch.int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2080, 1457, 2046,  330, 1291, 1827,    3,    0,    1,  291, 2134,  291,\n",
       "        1183,  811,  936,  524, 1336,    1,    0,    1, 2095, 1579, 1201,  302,\n",
       "         111,  432,    1, 1410, 2495, 2072, 1963, 1107, 2481, 2629,  971,    1,\n",
       "           0,    1, 1008,  606,  112, 1798, 2192, 1528,    3,    0,    1, 2112,\n",
       "        2380, 1586, 1921, 1116,  758, 2522,  520,    1,    0,    1,  359, 1939,\n",
       "        1022,  592, 2085, 2537,    3,    0,    1, 1528, 2161,  378, 1067,  324,\n",
       "        2050, 1823, 2380,    1,    0,    1, 1690, 1457,  529, 2188, 2042, 1091,\n",
       "           3,    0,    1,  119, 1555, 1568, 1037,    3,    0,  644,  860, 2372,\n",
       "        2289,    1,    0,    1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let now encode the entire text dataset and store it into torch.Tensor\n",
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "\n",
    "data[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e8f050a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31542, 644)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now split up the data into train set and validation set\n",
    "n = round(len(data) * 0.98);\n",
    "train_data = data[:n]\n",
    "val_data   = data[n:]\n",
    "len(train_data), len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b416e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split='train'):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(train_data) - block_size, (batch_size, ))\n",
    "    xb = torch.stack([train_data[i:i+block_size] for i in ix])\n",
    "    yb = torch.stack([train_data[i+1:i+1+block_size] for i in ix])\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "    return xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35c30bff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "class MultipleHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.num_heads = num_heads\n",
    "        self.query = nn.Linear(C, head_size * num_heads)\n",
    "        self.key = nn.Linear(C, head_size * num_heads)\n",
    "        self.value = nn.Linear(C, head_size * num_heads)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(T, T)))\n",
    "        self.proj = nn.Linear(num_heads*head_size, num_heads*head_size)\n",
    "        self.dropout1 = nn.Dropout(drop_out)\n",
    "        self.dropout2 = nn.Dropout(drop_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x = x.view(B, 1, T, C)\n",
    "        q = self.query(x).view(B, T, self.num_heads, self.head_size).transpose(1,2) # B, num_heads, T, head_size\n",
    "        k = self.key(x).view(B, T, self.num_heads, self.head_size).transpose(1,2) # B, num_heads, T, head_size\n",
    "        v = self.value(x).view(B, T, self.num_heads, self.head_size).transpose(1,2) # B, num_heads, T, head_size\n",
    "        # computer attention score\n",
    "        wei = q @ v.transpose(-2, -1) * self.head_size ** -0.5 # (B, num_heads, T, head_size) x (B, num_heads, head_size, T) -> (B, num_heads, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = torch.softmax(wei,dim=-1) # (B, num_heads, T, T)\n",
    "        wei = self.dropout1(wei)\n",
    "        # perform the weighted aggregation\n",
    "        out = wei@v # (B, num_heads, T, T) x (B, num_heads, T, head_size) -> (B, num_heads, T, head_size)\n",
    "        out = out.transpose(1,2).reshape(B, T, -1) # B, T, head_size * n_head\n",
    "        out = self.dropout2(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "                        nn.Linear(n_embd, n_embd * 4),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(n_embd * 4, n_embd),\n",
    "                        nn.Dropout(drop_out)\n",
    "                    )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, num_heads, n_embd):\n",
    "        super().__init__()\n",
    "        head_size = n_embd//num_heads\n",
    "        self.sa_head = MultipleHeadAttention(num_heads, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    def forward(self, x):\n",
    "        x = self.sa_head(self.ln1(x)) + x\n",
    "        x = self.ffwd(self.ln2(x)) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36427143",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    out = {}\n",
    "    for split in ['train', 'val']:\n",
    "        for i in range(eval_iters):\n",
    "            xb, yb = get_batch()\n",
    "            loss, logits = model(xb, yb)\n",
    "            losses[i] = loss\n",
    "        out[split] = losses.mean().item()\n",
    "    return out\n",
    "\n",
    "# bigram language model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(T, n_embd)\n",
    "        self.blocks = nn.Sequential(*nn.ModuleList([Block(num_heads, n_embd) for _ in range(num_blocks)] + [nn.LayerNorm(n_embd)]))\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T).to(device))\n",
    "        x = tok_emb + pos_emb # (B, T, n_embd)\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x) # (B, n_embd, vocab_size) x (B, T, n_embd) -> (B, T, vocab_size)\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        return loss, logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_token):\n",
    "        for i in range(max_new_token):\n",
    "            loss, logits = self(idx[:, -block_size:])\n",
    "            logits = logits[:, -1,:]\n",
    "            probs = F.softmax(logits, -1)\n",
    "            next_idx = torch.multinomial(probs, 1)\n",
    "            idx = torch.cat([idx, next_idx], 1)\n",
    "        return idx\n",
    "\n",
    "def train(lr=0.001, model_name=None, only_load_model=False):\n",
    "    optimizer = AdamW(model.parameters())\n",
    "    out_dir = Path('../checkpoints')\n",
    "    fn = out_dir/model_name\n",
    "    if fn.is_file():\n",
    "        checkpoint = torch.load(fn, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    if only_load_model == False:\n",
    "        for i in range(max_iter + 1):\n",
    "            if (i % eval_iters == 0) and (i > 0):\n",
    "                out = estimate_loss()\n",
    "                print(f\"Train loss: {out['train']}. Val loss: {out['val']}. \")\n",
    "\n",
    "                # save checkpoint\n",
    "                checkpoint = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict()\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, fn)\n",
    "            xb, yb = get_batch()\n",
    "            loss, logits = model(xb, yb)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1325f488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64]) torch.Size([64, 64])\n",
      "0.293041 M parameters\n",
      "tensor(7.9529, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "B, T, C = 64, 64, 36 ; # batch, time, channel\n",
    "n_embd = C\n",
    "batch_size, block_size = B, T\n",
    "max_iter = 25000\n",
    "num_heads = 6\n",
    "num_blocks = 6\n",
    "eval_iters = 1000\n",
    "drop_out = 0.2\n",
    "head_size = n_embd / num_heads\n",
    "xb, yb = get_batch()\n",
    "lr = 0.0001\n",
    "print(xb.shape, yb.shape)\n",
    "\n",
    "model = BigramLanguageModel(vocab_size).to(device)\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "loss, logits = model(xb, yb)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3cde6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.715602397918701. Val loss: 3.7170333862304688. \n",
      "saving checkpoint to ../checkpoints\n",
      "Train loss: 2.451805353164673. Val loss: 2.4547319412231445. \n",
      "saving checkpoint to ../checkpoints\n",
      "Train loss: 1.6461000442504883. Val loss: 1.6471295356750488. \n",
      "saving checkpoint to ../checkpoints\n",
      "Train loss: 1.167556881904602. Val loss: 1.166641116142273. \n",
      "saving checkpoint to ../checkpoints\n",
      "Train loss: 0.8847333192825317. Val loss: 0.8847111463546753. \n",
      "saving checkpoint to ../checkpoints\n",
      "Train loss: 0.7004756927490234. Val loss: 0.6995299458503723. \n",
      "saving checkpoint to ../checkpoints\n",
      "Train loss: 0.609959602355957. Val loss: 0.6089354157447815. \n",
      "saving checkpoint to ../checkpoints\n",
      "Train loss: 0.5567530393600464. Val loss: 0.5564932227134705. \n",
      "saving checkpoint to ../checkpoints\n",
      "Train loss: 0.4830600321292877. Val loss: 0.48131075501441956. \n",
      "saving checkpoint to ../checkpoints\n",
      "Train loss: 0.45059171319007874. Val loss: 0.450531929731369. \n",
      "saving checkpoint to ../checkpoints\n",
      "Train loss: 0.42473775148391724. Val loss: 0.42357054352760315. \n",
      "saving checkpoint to ../checkpoints\n",
      "Train loss: 0.40449753403663635. Val loss: 0.4051833748817444. \n",
      "saving checkpoint to ../checkpoints\n",
      "Train loss: 0.39978060126304626. Val loss: 0.4000585973262787. \n",
      "saving checkpoint to ../checkpoints\n",
      "Train loss: 0.3728296756744385. Val loss: 0.3725215494632721. \n",
      "saving checkpoint to ../checkpoints\n",
      "Train loss: 0.36801204085350037. Val loss: 0.36851534247398376. \n",
      "saving checkpoint to ../checkpoints\n",
      "Train loss: 0.3432505428791046. Val loss: 0.34522923827171326. \n",
      "saving checkpoint to ../checkpoints\n",
      "Train loss: 0.33629897236824036. Val loss: 0.33626800775527954. \n",
      "saving checkpoint to ../checkpoints\n",
      "Train loss: 0.33941349387168884. Val loss: 0.34081289172172546. \n",
      "saving checkpoint to ../checkpoints\n",
      "Train loss: 0.3141455352306366. Val loss: 0.31441447138786316. \n",
      "saving checkpoint to ../checkpoints\n",
      "Train loss: 0.3109095096588135. Val loss: 0.31082460284233093. \n",
      "saving checkpoint to ../checkpoints\n",
      "Train loss: 0.29889073967933655. Val loss: 0.2982620596885681. \n",
      "saving checkpoint to ../checkpoints\n",
      "Train loss: 0.29737386107444763. Val loss: 0.29717889428138733. \n",
      "saving checkpoint to ../checkpoints\n",
      "Train loss: 0.2884153425693512. Val loss: 0.2893764078617096. \n",
      "saving checkpoint to ../checkpoints\n",
      "Train loss: 0.2856851816177368. Val loss: 0.28586092591285706. \n",
      "saving checkpoint to ../checkpoints\n",
      "Train loss: 0.2757451832294464. Val loss: 0.27520686388015747. \n",
      "saving checkpoint to ../checkpoints\n"
     ]
    }
   ],
   "source": [
    "train(lr=lr, model_name='combine.pt_v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "452a222f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " mắc lừa lọc đã dành có nơi \n",
      "  \n",
      " rõ ràng mở mắt đầy xưa nay ,  \n",
      " thầm một chiều quyền xót xa \n",
      "  \n",
      " gặp từ hương lửa tàn ,  \n",
      " thật tin nghe hẳn nghìn sầu \n",
      "  chia vò hồng nhan ,  \n",
      " khách hồng rụng một lời gửi \n",
      "  \n",
      " vân trăng nọ hoa đào ,  \n",
      " lòng kia giữ giàng họ thúc một xa \n",
      "  \n",
      " dâng thư trước đã thẹn nàng ,  \n",
      " khóc người thấy bóng trăng hoa? \n",
      " mặt nào ai có hôm ngồi ,  \n",
      " là nhiều sao nói cười như không \n",
      "  \n",
      " vỗ nay trong miệng những ngày đào ,  \n",
      " thương sao hẳn thành con sai quan thì \n",
      "  \n",
      " rằng sông cũng bớt vài trên một lời ,  \n",
      " ngẫm những gạt lệ ,  \n",
      " ngập ngừng lại gieo lấy mình xa \n",
      "  \n",
      " đàn khoan bắt quì ,  \n",
      " sinh đà gieo vàng chờ được nào! \n",
      " tơ vì nước đến sau ,  \n",
      " cơ từ đã thu ngại công \n",
      "  \n",
      " non người quốc sắc nước non ,  \n",
      " tiếc thay huyên rõ ràng đó luồn đây \n",
      "  \n",
      " thầm lời đã sống đọa theo sau \n",
      "  \n",
      " thương nàng báo đáp ân tình ,  \n",
      " chút nàng ba sinh nàng ra xin đi! \n",
      " từ rằng: nghề mọn nhà ,  \n",
      " lòng thề nọ ngẩn ngơ ngẩn ngơ ngẩn sầu \n",
      "  \n",
      " bóng tà tà dâu ,  \n",
      " bóng tà tà đã ra phụ phàng ,  \n",
      " tiểu thư đã áp thẳng tìm tòi ngẩn ngơ \n",
      "  \n",
      " tơi am mây tạnh là ,  \n",
      " thiết quân có mụ vì nhà thường ,  \n",
      " lửa phiền càng dập càng khêu mối phiền \n",
      "  \n",
      " một lòng trong tiền văn lão cũng chôn sương ,  \n",
      " giãi lòng: \n",
      " nhớ nơi mang những gạt vững chiều thần ,  \n",
      " hoa trôi dần dần mà đến đây? \n",
      " êm ả ghềnh sẵn hai bề vẹn hai! \n",
      " thôi ta sẽ chớ tình máu không \n",
      "  \n",
      " ngẫm duyên ta có mọi đồ ,  \n",
      " đã buồn cả ruột ngàn đã lề \n",
      "  \n",
      " nghề chơi lại càng dào mạch tương \n",
      "  \n",
      " đòi phen đổi mai ao ,  \n",
      " đủ ngần quả kiếp người đây ,  \n",
      " mười phần oanh \n",
      " hai văn hơi khéo vỡ hai ,  \n",
      " giắt mặt mà liễu cờ mấy khi! \n",
      " hoa bèo nữa vàng sắm sửa muộn xưa \n",
      "  \n",
      " có điều ngang ngửa vì này bèo ,  \n",
      " lòng sâu nghĩa người nhỏ xôn xao \n",
      "  \n",
      " người nách thước còn ngần này ,  biết trong quân chầy \n",
      "  \n",
      " hoa truyền sửa lần mơ chay lòng ,  \n",
      " tìm\n"
     ]
    }
   ],
   "source": [
    "sent = model.generate(idx=torch.zeros((1,1), dtype=torch.long).to(device), max_new_token=500)\n",
    "print(decode(sent[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62bef649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " chàng về viện sách nàng dời lầu trang \n",
      "  \n",
      "  \n",
      " từ phen đá biết tuổi vàng ,  \n",
      " tình càng vén vì hoa kề ,  \n",
      " mấy lòng vừa ghé xiêu xiêu xiêu xiêu xiêu \n",
      "  \n",
      " vài tuần bạc ác sầu cho phu \n",
      "  \n",
      " xuân nước dẫy sóng đủ đường ,  \n",
      " phép về xuân dù phường chia hai \n",
      "  \n",
      " những là một lần mới ra ,  \n",
      " chàng càng trông tỏ thức hồng ,  \n",
      " rành rành tích việt duyên ngồi \n",
      "  \n",
      "  \n",
      " lấy điều trúc lục e lệ ,  \n",
      " khóc than ngọc cho nàng tình đầu ,  \n",
      " thẹn mình chén gặp nàng cần dịu dàng! \n",
      " mụ tháng thật quẩn trà cây \n",
      "  \n",
      " mảnh người dưới nguyệt thân ,  \n",
      " chàng vương nghe tiếng vàng liêu bưng kín chẳng ưa? \n",
      "  \n",
      " gia hoa đào khuya khăn ,  \n",
      " đất bằng ăn ngày một đau \n",
      "  \n",
      " nhớ nơi hằng thủy mai sau! \n",
      " những là nặng nắng mưa ,  \n",
      " buồng không thương chi cho khi về lầu xanh \n",
      "  \n",
      " rằng sông chẳng chút cũ tràng oanh ,  \n",
      " uốn lưng bút giá dày đã đành ,  \n",
      " gấp người còn có ai sở trác giữa trời \n",
      "  \n",
      " bắt về đến kim ,  \n",
      " mụ quản huyền đâu đã giục đành ,  \n",
      " chiều lòng biết có nợ chiều đời \n",
      "  \n",
      " gieo trời cạn ý đà sương được lời \n",
      "  \n",
      " tình nhân mới hạ công ,  \n",
      " còn nhiều đã có gương truyền hôm nay! \n",
      " tinh trướng nghe nói chẳng lựa sẻ lửa nhân ,  \n",
      " vân rằng: ái khỏi kiến lửa ba \n",
      "  \n",
      " cửa đóng then nhặt gói về lầu \n",
      "  \n",
      " mặn bất ý rụt rè ,  \n",
      " hoa kia đã chắp áo dài ,  \n",
      " xót liễu nước chưa vẹn chữ đồng tự hôn \n",
      "  \n",
      " cải nhậm hương lân nồi trời \n",
      "  \n",
      " sự nhà hành viện xưa nay ,  \n",
      " cũng đâu đã về chia cao \n",
      "  \n",
      " bao nhiêu đoạn khổ ,  tình chẳng treo trên \n",
      "  \n",
      " sự tình chàng thúc một tỉnh say ,  \n",
      " tin sương gieo xuống bóng cờ mấy hồi khốn hay \n",
      "  \n",
      " đầy sông tiền riêng chưa nện cầu vắng đâu \n",
      "  \n",
      " sợ lần khân quá ra ,  \n",
      " đây vì hoa cuối xuân đường trần hình ,  \n",
      " một phen đá biết là lỡ sinh \n",
      "  \n",
      " giác duyên ngắn chén tài hoa tự tôi \n",
      "  \n",
      " hơn người ngồi dai ,  \n",
      " nhìn nàng ra nặng nào thôi đền tình \n",
      "  \n",
      " khuyển thơ ngây thơ ngây thơ ngây \n",
      "  \n",
      " thoạt gánh như nung gan\n"
     ]
    }
   ],
   "source": [
    "sent = model.generate(idx=torch.zeros((1,1), dtype=torch.long).to(device), max_new_token=500)\n",
    "print(decode(sent[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a0322f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mây trôi bèo dạt đã đành ,  \n",
      " lại càng đứng lặng nhìn được điều \n",
      "  \n",
      " giọng kiều rền rĩ trướng loan ,  \n",
      " nhà huyên chợt sinh? \n",
      " bàn ngần ngọn vì sự bất xưa \n",
      " sá đá tài trong ,  \n",
      " giở đồ chuông khánh các lạ đời \n",
      "  \n",
      " tâm thu ,  \n",
      " khi vào điều của dây thường ,  \n",
      " lập năm bể mới hay không? \n",
      " sâm thương bằng tiện ở bắc mặn \n",
      "  \n",
      " ghế quanh những khan giọng tình ,  \n",
      " dập dìu bỗng khuôn đó cầm! \n",
      " thời làng đình nghe hiếu tâm ,  \n",
      " ba bề vẹn một nhà thì nên bay bất kỳ ,  \n",
      " xôn xao ngoài hoặc có xuân đường vân mới giãi chiều \n",
      "  \n",
      " thưa rằng: sắc lâm truy ,  \n",
      " sắc đành đã\n"
     ]
    }
   ],
   "source": [
    "sent = model.generate(torch.tensor([encode(\"mây trôi\".split(\" \"))]).to(device), max_new_token=150)\n",
    "print(decode(sent[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46605650",
   "metadata": {},
   "source": [
    "### Convert this file to md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ae96919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import Javascript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "499ce9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('this_notebook = \"' + IPython.notebook.notebook_name + '\"')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "IPython.notebook.kernel.execute('this_notebook = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03aa8aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-02-08-train-GPT-on-vietnamese-dataset.ipynb'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41784204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook 2023-02-08-train-GPT-on-vietnamese-dataset.ipynb to markdown\n",
      "[NbConvertApp] Writing 19749 bytes to ../_posts/2023-02-08-train-GPT-on-vietnamese-dataset.md\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to markdown {this_notebook} --output-dir=../_posts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
